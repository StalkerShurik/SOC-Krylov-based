Namespace(batch_size=128, epochs=100, lr_min=0.0, lr_max=0.1, weight_decay=0.0005, momentum=0.9, gamma=0.0, opt_level='O0', loss_scale='1.0', conv_layer='soc', init_channels=32, activation='maxmin', block_size=1, lln=False, data_dir='./cifar-data', dataset='cifar10', epsilon=36, out_dir='LipConvnet_cifar10_1_soc_32_maxmin_cr0.0', seed=0)
Epoch 	 Seconds 	 LR 	 Train Loss 	 Train Acc 	 Test Loss 	 Test Acc 	 Test Robust (36) 	 Test Robust (72) 	 Test Robust (108) 	 Test Cert
0 	 98.0 	 87.9 	 0.1000 	 1.8076 	 0.3528 	 1.4540 	 0.4822 	 0.2414 	 0.0921 	 0.0257 	 0.1714
1 	 99.4 	 87.8 	 0.1000 	 1.5088 	 0.4603 	 1.3591 	 0.5173 	 0.3046 	 0.1459 	 0.0581 	 0.2109
2 	 95.4 	 85.8 	 0.1000 	 1.4209 	 0.4950 	 1.2739 	 0.5490 	 0.3557 	 0.1866 	 0.0807 	 0.2357
3 	 105.7 	 96.3 	 0.1000 	 1.3700 	 0.5168 	 1.2270 	 0.5726 	 0.3726 	 0.2109 	 0.1028 	 0.2493
4 	 105.0 	 94.9 	 0.1000 	 1.3115 	 0.5422 	 1.2414 	 0.5683 	 0.3471 	 0.1879 	 0.0931 	 0.2373
5 	 110.1 	 98.8 	 0.1000 	 1.2804 	 0.5530 	 1.2026 	 0.5798 	 0.3857 	 0.2119 	 0.0917 	 0.2436
6 	 119.7 	 108.3 	 0.1000 	 1.2559 	 0.5647 	 1.1738 	 0.5859 	 0.4001 	 0.2362 	 0.1162 	 0.2651
7 	 121.5 	 109.6 	 0.1000 	 1.2382 	 0.5696 	 1.1275 	 0.6128 	 0.4175 	 0.2439 	 0.1281 	 0.2662
8 	 121.5 	 110.2 	 0.1000 	 1.2180 	 0.5760 	 1.1539 	 0.6038 	 0.4000 	 0.2206 	 0.1022 	 0.2473
9 	 119.8 	 108.3 	 0.1000 	 1.2082 	 0.5831 	 1.1179 	 0.6161 	 0.4256 	 0.2464 	 0.1221 	 0.2646
10 	 120.1 	 108.5 	 0.1000 	 1.2001 	 0.5864 	 1.1182 	 0.6161 	 0.4224 	 0.2498 	 0.1264 	 0.2678
11 	 119.2 	 108.0 	 0.1000 	 1.1852 	 0.5927 	 1.0693 	 0.6417 	 0.4403 	 0.2624 	 0.1372 	 0.2710
12 	 115.0 	 104.1 	 0.1000 	 1.1768 	 0.5946 	 1.0905 	 0.6338 	 0.4289 	 0.2548 	 0.1352 	 0.2699
13 	 115.7 	 104.7 	 0.1000 	 1.1793 	 0.5945 	 1.0992 	 0.6257 	 0.4427 	 0.2756 	 0.1479 	 0.2871
14 	 116.1 	 105.1 	 0.1000 	 1.1743 	 0.5977 	 1.1241 	 0.6086 	 0.4243 	 0.2595 	 0.1448 	 0.2829
15 	 114.1 	 104.6 	 0.1000 	 1.1630 	 0.6025 	 1.1086 	 0.6116 	 0.4347 	 0.2716 	 0.1541 	 0.2923
16 	 103.0 	 92.1 	 0.1000 	 1.1593 	 0.6051 	 1.1619 	 0.5997 	 0.4410 	 0.2857 	 0.1600 	 0.3044
17 	 107.1 	 96.3 	 0.1000 	 1.1602 	 0.6027 	 1.0564 	 0.6407 	 0.4524 	 0.2760 	 0.1393 	 0.2772
18 	 112.3 	 100.5 	 0.1000 	 1.1543 	 0.6051 	 1.2332 	 0.5677 	 0.4136 	 0.2755 	 0.1665 	 0.3183
19 	 123.1 	 111.4 	 0.1000 	 1.1466 	 0.6103 	 1.0451 	 0.6496 	 0.4552 	 0.2798 	 0.1501 	 0.2822
20 	 122.6 	 110.8 	 0.1000 	 1.1391 	 0.6120 	 1.0594 	 0.6361 	 0.4624 	 0.2966 	 0.1596 	 0.2978
21 	 121.2 	 110.0 	 0.1000 	 1.1405 	 0.6109 	 1.1004 	 0.6240 	 0.4544 	 0.2896 	 0.1599 	 0.2968
22 	 117.3 	 106.1 	 0.1000 	 1.1414 	 0.6118 	 1.0698 	 0.6308 	 0.4560 	 0.2888 	 0.1582 	 0.2936
23 	 117.0 	 105.6 	 0.1000 	 1.1370 	 0.6133 	 1.0509 	 0.6444 	 0.4478 	 0.2652 	 0.1385 	 0.2716
24 	 116.9 	 105.9 	 0.1000 	 1.1351 	 0.6109 	 1.0943 	 0.6330 	 0.4306 	 0.2605 	 0.1399 	 0.2746
25 	 116.9 	 105.7 	 0.1000 	 1.1315 	 0.6149 	 1.0855 	 0.6283 	 0.4366 	 0.2706 	 0.1446 	 0.2839
26 	 116.7 	 105.9 	 0.1000 	 1.1256 	 0.6165 	 1.0402 	 0.6514 	 0.4602 	 0.2861 	 0.1501 	 0.2862
27 	 116.5 	 105.7 	 0.1000 	 1.1291 	 0.6159 	 1.0661 	 0.6467 	 0.4524 	 0.2697 	 0.1381 	 0.2746
28 	 116.2 	 105.2 	 0.1000 	 1.1224 	 0.6170 	 1.0671 	 0.6458 	 0.4469 	 0.2709 	 0.1480 	 0.2822
29 	 117.2 	 105.8 	 0.1000 	 1.1227 	 0.6170 	 1.0673 	 0.6374 	 0.4453 	 0.2785 	 0.1539 	 0.2881
30 	 116.5 	 105.5 	 0.1000 	 1.1216 	 0.6187 	 1.0394 	 0.6419 	 0.4524 	 0.2894 	 0.1612 	 0.2913
31 	 116.5 	 105.5 	 0.1000 	 1.1181 	 0.6199 	 1.0541 	 0.6472 	 0.4434 	 0.2772 	 0.1436 	 0.2775
32 	 115.8 	 104.8 	 0.1000 	 1.1169 	 0.6211 	 1.0675 	 0.6391 	 0.4504 	 0.2771 	 0.1523 	 0.2895
33 	 116.9 	 105.8 	 0.1000 	 1.1149 	 0.6215 	 1.0558 	 0.6380 	 0.4527 	 0.2847 	 0.1489 	 0.2837
34 	 116.9 	 105.6 	 0.1000 	 1.1186 	 0.6223 	 1.0455 	 0.6486 	 0.4681 	 0.3012 	 0.1711 	 0.3026
35 	 104.2 	 95.2 	 0.1000 	 1.1128 	 0.6227 	 1.0743 	 0.6324 	 0.4562 	 0.2885 	 0.1669 	 0.3021
